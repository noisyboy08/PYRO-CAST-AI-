{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8512b5b8",
   "metadata": {},
   "source": [
    "# 02 - Data Preprocessing & Feature Engineering\n",
    "## Wildfire Risk Prediction Project\n",
    "\n",
    "This notebook handles data preprocessing, feature engineering, and data preparation for ML models.\n",
    "We'll clean the data, create new features, and prepare train/test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ec26c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.13.5)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"d:/New folder/proj-/venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "print(\"ğŸ”§ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735085dc",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db096356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw dataset\n",
    "df = pd.read_csv('../backend/data/raw/wildfire_dataset.csv')\n",
    "\n",
    "print(f\"ğŸ“Š Dataset loaded: {df.shape}\")\n",
    "print(f\"ğŸ“‹ Columns: {list(df.columns)}\")\n",
    "print(f\"\\nğŸ¯ Target variable distribution:\")\n",
    "print(df['occured'].value_counts())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a107846",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6350a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"ğŸ” Missing Values Analysis:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "# Handle missing values if any\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"\\nğŸ› ï¸ Handling missing values...\")\n",
    "    \n",
    "    # Numeric columns: fill with median\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "    \n",
    "    print(\"âœ… Missing values handled with median imputation\")\n",
    "else:\n",
    "    print(\"âœ… No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6133ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"ğŸ” Duplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(f\"ğŸ§¹ Removing {duplicates} duplicate rows...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"âœ… Dataset shape after removing duplicates: {df.shape}\")\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823b76f9",
   "metadata": {},
   "source": [
    "## Outlier Detection and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb34849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_cap_outliers(df, column, method='iqr', cap_method='clip'):\n",
    "    \"\"\"\n",
    "    Detect and handle outliers using IQR or Z-score method\n",
    "    \"\"\"\n",
    "    if method == 'iqr':\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "    elif method == 'zscore':\n",
    "        mean = df[column].mean()\n",
    "        std = df[column].std()\n",
    "        lower_bound = mean - 3 * std\n",
    "        upper_bound = mean + 3 * std\n",
    "    \n",
    "    # Count outliers\n",
    "    outliers_count = ((df[column] < lower_bound) | (df[column] > upper_bound)).sum()\n",
    "    \n",
    "    if cap_method == 'clip' and outliers_count > 0:\n",
    "        df[column] = np.clip(df[column], lower_bound, upper_bound)\n",
    "    \n",
    "    return outliers_count, lower_bound, upper_bound\n",
    "\n",
    "# Apply outlier treatment to key features\n",
    "key_features = ['temp_mean', 'humidity_min', 'wind_speed_max', 'pressure_mean', 'fire_weather_index']\n",
    "outlier_summary = []\n",
    "\n",
    "print(\"ğŸ¯ Outlier Detection and Treatment:\")\n",
    "for feature in key_features:\n",
    "    if feature in df.columns:\n",
    "        outliers_count, lower, upper = detect_and_cap_outliers(df, feature, method='zscore')\n",
    "        outlier_summary.append({\n",
    "            'Feature': feature,\n",
    "            'Outliers Capped': outliers_count,\n",
    "            'Lower Bound': f\"{lower:.2f}\",\n",
    "            'Upper Bound': f\"{upper:.2f}\"\n",
    "        })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(outlier_df)\n",
    "print(f\"\\nâœ… Outliers handled using 3-sigma rule (Z-score method)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f077d",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”¬ Creating new features based on domain knowledge...\")\n",
    "\n",
    "# 1. Heat-Humidity Interaction\n",
    "if 'temp_mean' in df.columns and 'humidity_min' in df.columns:\n",
    "    df['heat_humidity_interaction'] = df['temp_mean'] * (100 - df['humidity_min']) / 100\n",
    "    print(\"âœ… Created heat_humidity_interaction feature\")\n",
    "\n",
    "# 2. Dryness-Wind Factor (fire spread potential)\n",
    "if 'wind_speed_max' in df.columns and 'humidity_min' in df.columns:\n",
    "    df['dryness_wind_factor'] = (100 - df['humidity_min']) * df['wind_speed_max'] / 100\n",
    "    print(\"âœ… Created dryness_wind_factor feature\")\n",
    "\n",
    "# 3. Temperature Range Features\n",
    "if 'temp_mean' in df.columns and 'temp_range' in df.columns:\n",
    "    df['temp_max_approx'] = df['temp_mean'] + df['temp_range'] / 2\n",
    "    df['temp_min_approx'] = df['temp_mean'] - df['temp_range'] / 2\n",
    "    print(\"âœ… Created temperature range features\")\n",
    "\n",
    "# 4. Fire Weather Index Categories\n",
    "if 'fire_weather_index' in df.columns:\n",
    "    df['fwi_category'] = pd.cut(\n",
    "        df['fire_weather_index'],\n",
    "        bins=[-np.inf, 5, 10, 20, np.inf],\n",
    "        labels=[1, 2, 3, 4]  # Low, Medium, High, Extreme\n",
    "    )\n",
    "    df['fwi_category'] = df['fwi_category'].astype(int)\n",
    "    print(\"âœ… Created FWI category feature\")\n",
    "\n",
    "# 5. Pressure Deviation (how far from standard pressure)\n",
    "if 'pressure_mean' in df.columns:\n",
    "    standard_pressure = 1013.25  # hPa at sea level\n",
    "    df['pressure_deviation'] = abs(df['pressure_mean'] - standard_pressure)\n",
    "    print(\"âœ… Created pressure deviation feature\")\n",
    "\n",
    "# 6. Wind Direction Consistency\n",
    "if 'wind_direction_std' in df.columns:\n",
    "    df['wind_consistency'] = 1 / (df['wind_direction_std'] + 1)  # Higher value = more consistent\n",
    "    print(\"âœ… Created wind consistency feature\")\n",
    "\n",
    "# 7. Evaporation Rate Proxy\n",
    "if 'evapotranspiration_total' in df.columns and 'humidity_min' in df.columns:\n",
    "    df['evaporation_rate'] = df['evapotranspiration_total'] / (df['humidity_min'] + 1)\n",
    "    print(\"âœ… Created evaporation rate proxy\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset shape after feature engineering: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e51c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display correlation of new features with target\n",
    "new_features = [\n",
    "    'heat_humidity_interaction', 'dryness_wind_factor', 'temp_max_approx', \n",
    "    'temp_min_approx', 'fwi_category', 'pressure_deviation', \n",
    "    'wind_consistency', 'evaporation_rate'\n",
    "]\n",
    "\n",
    "existing_features = [f for f in new_features if f in df.columns]\n",
    "\n",
    "if existing_features:\n",
    "    feature_correlations = df[existing_features + ['occured']].corr()['occured'].abs().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nğŸ¯ New Feature Correlations with Target:\")\n",
    "    for feature in existing_features:\n",
    "        corr = feature_correlations[feature]\n",
    "        print(f\"  â€¢ {feature}: {corr:.4f}\")\n",
    "    \n",
    "    # Visualize correlations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    feature_correlations[existing_features].plot(kind='bar')\n",
    "    plt.title('ğŸ¯ New Feature Correlations with Fire Occurrence')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Absolute Correlation')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdcc858",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to exclude from modeling\n",
    "exclude_features = [\n",
    "    'occured',  # target variable\n",
    "    'frp',      # fire radiative power - result of fire, not predictor\n",
    "    'lat', 'lon'  # location - will be used separately for mapping\n",
    "]\n",
    "\n",
    "# Select features for modeling\n",
    "feature_columns = [col for col in df.columns if col not in exclude_features]\n",
    "print(f\"ğŸ¯ Selected {len(feature_columns)} features for modeling:\")\n",
    "for i, feature in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "# Prepare feature matrix and target vector\n",
    "X = df[feature_columns]\n",
    "y = df['occured']\n",
    "\n",
    "print(f\"\\nğŸ“Š Final dataset shapes:\")\n",
    "print(f\"  Features (X): {X.shape}\")\n",
    "print(f\"  Target (y): {y.shape}\")\n",
    "print(f\"  Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981dd571",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a902f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(\"âœ‚ï¸ Data splitting completed:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Class distribution:\")\n",
    "print(f\"  Training - Fire: {y_train.sum()} ({y_train.mean()*100:.1f}%), No Fire: {(y_train == 0).sum()} ({(y_train == 0).mean()*100:.1f}%)\")\n",
    "print(f\"  Test - Fire: {y_test.sum()} ({y_test.mean()*100:.1f}%), No Fire: {(y_test == 0).sum()} ({(y_test == 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981615fc",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit scaler on training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"âš–ï¸ Feature scaling completed with StandardScaler\")\n",
    "print(f\"  Mean of scaled training features: {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"  Std of scaled training features: {X_train_scaled.std().mean():.6f}\")\n",
    "\n",
    "# Show scaling statistics for key features\n",
    "print(f\"\\nğŸ“Š Scaling Statistics (first 5 features):\")\n",
    "for feature in X_train.columns[:5]:\n",
    "    original_mean = X_train[feature].mean()\n",
    "    original_std = X_train[feature].std()\n",
    "    scaled_mean = X_train_scaled[feature].mean()\n",
    "    scaled_std = X_train_scaled[feature].std()\n",
    "    \n",
    "    print(f\"  {feature}:\")\n",
    "    print(f\"    Original: Î¼={original_mean:.3f}, Ïƒ={original_std:.3f}\")\n",
    "    print(f\"    Scaled:   Î¼={scaled_mean:.3f}, Ïƒ={scaled_std:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09377ee9",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea74cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if it doesn't exist\n",
    "import os\n",
    "processed_dir = '../backend/data/processed/'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Save training data\n",
    "train_data = pd.concat([X_train_scaled, y_train], axis=1)\n",
    "train_data.to_csv(processed_dir + 'wildfire_train.csv', index=False)\n",
    "print(f\"âœ… Training data saved: {train_data.shape}\")\n",
    "\n",
    "# Save test data\n",
    "test_data = pd.concat([X_test_scaled, y_test], axis=1)\n",
    "test_data.to_csv(processed_dir + 'wildfire_test.csv', index=False)\n",
    "print(f\"âœ… Test data saved: {test_data.shape}\")\n",
    "\n",
    "# Save complete processed dataset\n",
    "all_scaled_X = pd.concat([X_train_scaled, X_test_scaled])\n",
    "all_y = pd.concat([y_train, y_test])\n",
    "complete_data = pd.concat([all_scaled_X, all_y], axis=1)\n",
    "complete_data.to_csv(processed_dir + 'wildfire_processed.csv', index=False)\n",
    "print(f\"âœ… Complete processed data saved: {complete_data.shape}\")\n",
    "\n",
    "# Save the scaler for future use\n",
    "joblib.dump(scaler, processed_dir + 'feature_scaler.pkl')\n",
    "print(\"âœ… Scaler saved for future use\")\n",
    "\n",
    "# Save feature names\n",
    "feature_info = {\n",
    "    'feature_columns': list(feature_columns),\n",
    "    'target_column': 'occured',\n",
    "    'excluded_columns': exclude_features,\n",
    "    'new_features': existing_features\n",
    "}\n",
    "joblib.dump(feature_info, processed_dir + 'feature_info.pkl')\n",
    "print(\"âœ… Feature information saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7d292",
   "metadata": {},
   "source": [
    "## Final Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c8b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final quality checks\n",
    "print(\"ğŸ” Final Data Quality Assessment:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(f\"âœ“ Training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"âœ“ Test data shape: {X_test_scaled.shape}\")\n",
    "print(f\"âœ“ No missing values in training: {X_train_scaled.isnull().sum().sum() == 0}\")\n",
    "print(f\"âœ“ No missing values in test: {X_test_scaled.isnull().sum().sum() == 0}\")\n",
    "print(f\"âœ“ Features properly scaled: {abs(X_train_scaled.mean().mean()) < 1e-10}\")\n",
    "print(f\"âœ“ Class balance maintained: Train={y_train.mean():.3f}, Test={y_test.mean():.3f}\")\n",
    "\n",
    "# Feature importance preview (correlation-based)\n",
    "feature_importance = X_train_scaled.corrwith(y_train).abs().sort_values(ascending=False)\n",
    "print(f\"\\nğŸ† Top 10 Most Correlated Features:\")\n",
    "for i, (feature, corr) in enumerate(feature_importance.head(10).items(), 1):\n",
    "    print(f\"  {i:2d}. {feature}: {corr:.4f}\")\n",
    "\n",
    "print(f\"\\nâœ… Preprocessing completed successfully!\")\n",
    "print(f\"ğŸ“ Processed files saved to: {processed_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb1312",
   "metadata": {},
   "source": [
    "## Visualization of Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7512e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of processed data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Distribution of scaled features (sample)\n",
    "sample_features = X_train_scaled.columns[:4]\n",
    "for i, feature in enumerate(sample_features):\n",
    "    row, col = i // 2, i % 2\n",
    "    axes[row, col].hist(X_train_scaled[feature], bins=50, alpha=0.7, density=True)\n",
    "    axes[row, col].set_title(f'Distribution of Scaled {feature}')\n",
    "    axes[row, col].set_xlabel('Scaled Value')\n",
    "    axes[row, col].set_ylabel('Density')\n",
    "    axes[row, col].axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.suptitle('ğŸ“Š Sample Feature Distributions After Scaling', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Correlation heatmap of top features\n",
    "top_features = feature_importance.head(10).index.tolist()\n",
    "correlation_matrix = X_train_scaled[top_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            mask=mask,\n",
    "            cmap='RdYlBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.3f')\n",
    "plt.title('ğŸ”— Correlation Matrix of Top 10 Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Visualization completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
